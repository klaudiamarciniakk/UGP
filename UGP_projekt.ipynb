{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "30GDbaOY2Wlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b70042-5b64-4a2e-c06e-76ce7be242ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[torch]==4.34.1\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.14.1\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.99\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.14.7\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.4.1\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu==2.3.2\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (2.1.0+cu121)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch]==4.34.1)\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch]==4.34.1)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.7)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (3.4.1)\n",
            "Collecting multiprocess (from datasets==2.14.7)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (3.9.1)\n",
            "Collecting responses<0.19 (from evaluate==0.4.1)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting portalocker (from sacrebleu==2.3.2)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.2) (0.9.0)\n",
            "Collecting colorama (from sacrebleu==2.3.2)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.2) (4.9.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]==4.34.1) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]==4.34.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.7) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.7) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (1.3.0)\n",
            "Installing collected packages: sentencepiece, portalocker, dill, colorama, sacrebleu, responses, multiprocess, huggingface-hub, tokenizers, accelerate, transformers, datasets, evaluate\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.2\n",
            "    Uninstalling huggingface-hub-0.20.2:\n",
            "      Successfully uninstalled huggingface-hub-0.20.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed accelerate-0.26.1 colorama-0.4.6 datasets-2.14.7 dill-0.3.7 evaluate-0.4.1 huggingface-hub-0.17.3 multiprocess-0.70.15 portalocker-2.8.2 responses-0.18.0 sacrebleu-2.3.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ],
      "source": [
        "pip install transformers[torch]==4.34.1 tokenizers==0.14.1 sentencepiece==0.1.99 datasets==2.14.7 evaluate==0.4.1 sacrebleu==2.3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!mkdir data\n",
        "!python prepare_emotion.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZW7zGc24vxG",
        "outputId": "7bf1e14f-8cab-432b-c9d8-1902bb77a14e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Loaded dataset emotion: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n",
            "INFO:__main__:Train:  16000\n",
            "INFO:__main__:Label 0:    581\n",
            "INFO:__main__:Label 1:    695\n",
            "INFO:__main__:Label 2:    159\n",
            "INFO:__main__:Label 3:    275\n",
            "INFO:__main__:Label 4:    224\n",
            "INFO:__main__:Label 5:     66\n",
            "INFO:__main__:Valid:    998\n",
            "INFO:__main__:Test :   1002\n",
            "INFO:__main__:Saving into: data/train.json\n",
            "INFO:__main__:Saved limited (5000) version in: data/train-5k.json\n",
            "INFO:__main__:Saving into: data/s2s-train.json\n",
            "INFO:__main__:Saved limited (5000) version in: data/s2s-train-5k.json\n",
            "INFO:__main__:Saving into: data/valid.json\n",
            "INFO:__main__:Saved limited (1998) version in: data/valid-5k.json\n",
            "INFO:__main__:Saving into: data/s2s-valid.json\n",
            "INFO:__main__:Saved limited (1998) version in: data/s2s-valid-5k.json\n",
            "INFO:__main__:Saving into: data/test.json\n",
            "INFO:__main__:Saved limited (2006) version in: data/test-5k.json\n",
            "INFO:__main__:Saving into: data/s2s-test.json\n",
            "INFO:__main__:Saved limited (2006) version in: data/s2s-test-5k.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkjCcQzy8AdH",
        "outputId": "26c6770e-9bd9-4cfc-cec0-49c537d060db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 14 20:04:26 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path gpt2 \\\n",
        "  --custom_model gpt2_simple \\\n",
        "  --train_file data/train-5k.json  \\\n",
        "  --validation_file data/valid-5k.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 1000 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 50 \\\n",
        "  --eval_steps 1000 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model 'accuracy' \\\n",
        "  --greater_is_better 'True' \\\n",
        "  --load_best_model_at_end 'True' \\\n",
        "  --report_to 'none' \\\n",
        "  --output_dir out/emotion/gpt2_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p8eeaKk8dJn",
        "outputId": "1d95a8fd-f4ce-4214-a98b-c3584d0c530d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-14 20:04:36.992193: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-14 20:04:36.992255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-14 20:04:36.993614: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-14 20:04:38.104302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/14/2024 20:04:42 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/14/2024 20:04:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/emotion/gpt2_simple/runs/Jan14_20-04-41_35868d6163fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=50,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=out/emotion/gpt2_simple,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/emotion/gpt2_simple,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "01/14/2024 20:04:42 - INFO - __main__ - load a local file for train: data/train-5k.json\n",
            "01/14/2024 20:04:42 - INFO - __main__ - load a local file for validation: data/valid-5k.json\n",
            "Using custom data configuration default-b1c162b574f6f092\n",
            "01/14/2024 20:04:42 - INFO - datasets.builder - Using custom data configuration default-b1c162b574f6f092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/14/2024 20:04:42 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "01/14/2024 20:04:42 - INFO - datasets.builder - Generating dataset json (/content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "01/14/2024 20:04:42 - INFO - datasets.builder - Downloading and preparing dataset json/default to /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 18766.46it/s]\n",
            "Downloading took 0.0 min\n",
            "01/14/2024 20:04:42 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "01/14/2024 20:04:42 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1975.65it/s]\n",
            "Generating train split\n",
            "01/14/2024 20:04:42 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 4999 examples [00:00, 450716.37 examples/s]\n",
            "Generating validation split\n",
            "01/14/2024 20:04:42 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 1274 examples [00:00, 529483.09 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "01/14/2024 20:04:42 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "01/14/2024 20:04:42 - INFO - datasets.builder - Dataset json downloaded and prepared to /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Downloading config.json: 100% 665/665 [00:00<00:00, 3.57MB/s]\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:04:42,872 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:04:42,873 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:550] 2024-01-14 20:04:42,974 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:04:43,075 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:04:43,076 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading vocab.json: 100% 1.04M/1.04M [00:00<00:00, 3.21MB/s]\n",
            "Downloading merges.txt: 100% 456k/456k [00:00<00:00, 1.83MB/s]\n",
            "Downloading tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.09MB/s]\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:04:44,941 >> loading file vocab.json from cache at .cache_training/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:04:44,942 >> loading file merges.txt from cache at .cache_training/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:04:44,942 >> loading file tokenizer.json from cache at .cache_training/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:04:44,942 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:04:44,942 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:04:44,942 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:04:44,942 >> loading configuration file config.json from cache at .cache_training/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:04:44,943 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "01/14/2024 20:04:45 - INFO - __main__ - Using hidden states in model: False\n",
            "01/14/2024 20:04:45 - INFO - __main__ - Using implementation from class: GPT2ForSequenceClassificationCustomSimple\n",
            "Downloading model.safetensors: 100% 548M/548M [00:02<00:00, 263MB/s]\n",
            "[INFO|modeling_utils.py:2993] 2024-01-14 20:04:47,329 >> loading weights file model.safetensors from cache at .cache_training/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors\n",
            "[INFO|modeling_utils.py:3775] 2024-01-14 20:04:49,730 >> All model checkpoint weights were used when initializing GPT2ForSequenceClassificationCustomSimple.\n",
            "\n",
            "[WARNING|modeling_utils.py:3777] 2024-01-14 20:04:49,731 >> Some weights of GPT2ForSequenceClassificationCustomSimple were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.dense_2.weight', 'score.dense_1.weight', 'score.dense_1.bias', 'score.dense_2.bias', 'score.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "01/14/2024 20:04:49 - INFO - __main__ - Set PAD token to EOS: <|endoftext|>\n",
            "Model: $GPT2ForSequenceClassificationCustomSimple(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (score): GPT2ClassificationHeadCustomSimple(\n",
            "    (dense_1): Linear(in_features=768, out_features=1536, bias=True)\n",
            "    (dense_2): Linear(in_features=1536, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_proj): Linear(in_features=768, out_features=6, bias=False)\n",
            "  )\n",
            ")\n",
            "Running tokenizer on dataset:   0% 0/4999 [00:00<?, ? examples/s]Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a84162f09e8af191.arrow\n",
            "01/14/2024 20:04:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-a84162f09e8af191.arrow\n",
            "Running tokenizer on dataset: 100% 4999/4999 [00:00<00:00, 8727.89 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1274 [00:00<?, ? examples/s]Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-79573621bbdb3ab3.arrow\n",
            "01/14/2024 20:04:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-79573621bbdb3ab3.arrow\n",
            "Running tokenizer on dataset: 100% 1274/1274 [00:00<00:00, 9360.47 examples/s]\n",
            "01/14/2024 20:04:50 - INFO - __main__ - Sample 912 of the training set: {'label': 2, 'text': 'i feel we need a little romantic boost in the relationship', 'input_ids': [72, 1254, 356, 761, 257, 1310, 14348, 5750, 287, 262, 2776, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/14/2024 20:04:50 - INFO - __main__ - Sample 204 of the training set: {'label': 1, 'text': 'i feel pretty mellow so far about whatever healing wounding process may be getting underway', 'input_ids': [72, 1254, 2495, 33748, 322, 523, 1290, 546, 4232, 11516, 40942, 1429, 743, 307, 1972, 17715, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/14/2024 20:04:50 - INFO - __main__ - Sample 2253 of the training set: {'label': 1, 'text': 'i feel ive answered those questions for her and shes pretty trusting for the most part', 'input_ids': [72, 1254, 220, 425, 9373, 883, 2683, 329, 607, 290, 673, 82, 2495, 33914, 329, 262, 749, 636, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 9.59MB/s]\n",
            "[INFO|trainer.py:761] 2024-01-14 20:04:52,513 >> The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassificationCustomSimple.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustomSimple.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1760] 2024-01-14 20:04:52,528 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2024-01-14 20:04:52,529 >>   Num examples = 4,999\n",
            "[INFO|trainer.py:1762] 2024-01-14 20:04:52,529 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1763] 2024-01-14 20:04:52,529 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1766] 2024-01-14 20:04:52,529 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1767] 2024-01-14 20:04:52,529 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1768] 2024-01-14 20:04:52,529 >>   Total optimization steps = 625\n",
            "[INFO|trainer.py:1769] 2024-01-14 20:04:52,530 >>   Number of trainable parameters = 126,806,016\n",
            "{'loss': 1.6874, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.08}\n",
            "{'loss': 1.6371, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.16}\n",
            "{'loss': 1.5751, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.24}\n",
            "{'loss': 1.4075, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.32}\n",
            "{'loss': 1.2843, 'learning_rate': 1.2e-05, 'epoch': 0.4}\n",
            "{'loss': 1.1892, 'learning_rate': 1.04e-05, 'epoch': 0.48}\n",
            "{'loss': 1.0472, 'learning_rate': 8.8e-06, 'epoch': 0.56}\n",
            "{'loss': 1.1155, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.64}\n",
            "{'loss': 1.0483, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.72}\n",
            "{'loss': 1.093, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
            "{'loss': 0.9219, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.88}\n",
            "{'loss': 0.9671, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.96}\n",
            "{'loss': 1.0031, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 625/625 [02:19<00:00,  4.60it/s][INFO|trainer.py:761] 2024-01-14 20:07:11,937 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustomSimple.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustomSimple.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3213] 2024-01-14 20:07:11,939 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2024-01-14 20:07:11,939 >>   Num examples = 1274\n",
            "[INFO|trainer.py:3218] 2024-01-14 20:07:11,939 >>   Batch size = 8\n",
            "\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/160 [00:00<00:06, 23.51it/s]\u001b[A\n",
            "  4% 6/160 [00:00<00:08, 18.17it/s]\u001b[A\n",
            "  5% 8/160 [00:00<00:08, 17.08it/s]\u001b[A\n",
            "  6% 10/160 [00:00<00:09, 16.60it/s]\u001b[A\n",
            "  8% 12/160 [00:00<00:09, 16.23it/s]\u001b[A\n",
            "  9% 14/160 [00:00<00:09, 15.92it/s]\u001b[A\n",
            " 10% 16/160 [00:00<00:09, 15.80it/s]\u001b[A\n",
            " 11% 18/160 [00:01<00:09, 15.71it/s]\u001b[A\n",
            " 12% 20/160 [00:01<00:08, 15.66it/s]\u001b[A\n",
            " 14% 22/160 [00:01<00:08, 15.73it/s]\u001b[A\n",
            " 15% 24/160 [00:01<00:08, 15.71it/s]\u001b[A\n",
            " 16% 26/160 [00:01<00:08, 15.67it/s]\u001b[A\n",
            " 18% 28/160 [00:01<00:08, 15.64it/s]\u001b[A\n",
            " 19% 30/160 [00:01<00:08, 15.62it/s]\u001b[A\n",
            " 20% 32/160 [00:01<00:08, 15.58it/s]\u001b[A\n",
            " 21% 34/160 [00:02<00:08, 15.57it/s]\u001b[A\n",
            " 22% 36/160 [00:02<00:07, 15.53it/s]\u001b[A\n",
            " 24% 38/160 [00:02<00:07, 15.53it/s]\u001b[A\n",
            " 25% 40/160 [00:02<00:07, 15.53it/s]\u001b[A\n",
            " 26% 42/160 [00:02<00:07, 15.36it/s]\u001b[A\n",
            " 28% 44/160 [00:02<00:07, 15.43it/s]\u001b[A\n",
            " 29% 46/160 [00:02<00:07, 15.31it/s]\u001b[A\n",
            " 30% 48/160 [00:03<00:07, 15.33it/s]\u001b[A\n",
            " 31% 50/160 [00:03<00:07, 15.32it/s]\u001b[A\n",
            " 32% 52/160 [00:03<00:07, 15.28it/s]\u001b[A\n",
            " 34% 54/160 [00:03<00:06, 15.24it/s]\u001b[A\n",
            " 35% 56/160 [00:03<00:06, 15.23it/s]\u001b[A\n",
            " 36% 58/160 [00:03<00:06, 15.25it/s]\u001b[A\n",
            " 38% 60/160 [00:03<00:06, 15.27it/s]\u001b[A\n",
            " 39% 62/160 [00:03<00:06, 15.28it/s]\u001b[A\n",
            " 40% 64/160 [00:04<00:06, 15.28it/s]\u001b[A\n",
            " 41% 66/160 [00:04<00:06, 15.33it/s]\u001b[A\n",
            " 42% 68/160 [00:04<00:06, 15.32it/s]\u001b[A\n",
            " 44% 70/160 [00:04<00:05, 15.30it/s]\u001b[A\n",
            " 45% 72/160 [00:04<00:05, 15.18it/s]\u001b[A\n",
            " 46% 74/160 [00:04<00:05, 15.29it/s]\u001b[A\n",
            " 48% 76/160 [00:04<00:05, 15.28it/s]\u001b[A\n",
            " 49% 78/160 [00:05<00:05, 15.06it/s]\u001b[A\n",
            " 50% 80/160 [00:05<00:05, 15.14it/s]\u001b[A\n",
            " 51% 82/160 [00:05<00:05, 15.17it/s]\u001b[A\n",
            " 52% 84/160 [00:05<00:05, 15.17it/s]\u001b[A\n",
            " 54% 86/160 [00:05<00:04, 15.16it/s]\u001b[A\n",
            " 55% 88/160 [00:05<00:04, 15.18it/s]\u001b[A\n",
            " 56% 90/160 [00:05<00:04, 15.34it/s]\u001b[A\n",
            " 57% 92/160 [00:05<00:04, 15.32it/s]\u001b[A\n",
            " 59% 94/160 [00:06<00:04, 15.41it/s]\u001b[A\n",
            " 60% 96/160 [00:06<00:04, 15.47it/s]\u001b[A\n",
            " 61% 98/160 [00:06<00:04, 15.46it/s]\u001b[A\n",
            " 62% 100/160 [00:06<00:03, 15.51it/s]\u001b[A\n",
            " 64% 102/160 [00:06<00:03, 15.49it/s]\u001b[A\n",
            " 65% 104/160 [00:06<00:03, 15.51it/s]\u001b[A\n",
            " 66% 106/160 [00:06<00:03, 15.54it/s]\u001b[A\n",
            " 68% 108/160 [00:06<00:03, 15.53it/s]\u001b[A\n",
            " 69% 110/160 [00:07<00:03, 15.53it/s]\u001b[A\n",
            " 70% 112/160 [00:07<00:03, 15.58it/s]\u001b[A\n",
            " 71% 114/160 [00:07<00:02, 15.45it/s]\u001b[A\n",
            " 72% 116/160 [00:07<00:02, 15.45it/s]\u001b[A\n",
            " 74% 118/160 [00:07<00:02, 15.44it/s]\u001b[A\n",
            " 75% 120/160 [00:07<00:02, 15.39it/s]\u001b[A\n",
            " 76% 122/160 [00:07<00:02, 15.44it/s]\u001b[A\n",
            " 78% 124/160 [00:07<00:02, 15.45it/s]\u001b[A\n",
            " 79% 126/160 [00:08<00:02, 15.48it/s]\u001b[A\n",
            " 80% 128/160 [00:08<00:02, 15.51it/s]\u001b[A\n",
            " 81% 130/160 [00:08<00:01, 15.51it/s]\u001b[A\n",
            " 82% 132/160 [00:08<00:01, 15.53it/s]\u001b[A\n",
            " 84% 134/160 [00:08<00:01, 15.46it/s]\u001b[A\n",
            " 85% 136/160 [00:08<00:01, 15.52it/s]\u001b[A\n",
            " 86% 138/160 [00:08<00:01, 15.58it/s]\u001b[A\n",
            " 88% 140/160 [00:09<00:01, 15.49it/s]\u001b[A\n",
            " 89% 142/160 [00:09<00:01, 15.50it/s]\u001b[A\n",
            " 90% 144/160 [00:09<00:01, 15.51it/s]\u001b[A\n",
            " 91% 146/160 [00:09<00:00, 15.52it/s]\u001b[A\n",
            " 92% 148/160 [00:09<00:00, 15.57it/s]\u001b[A\n",
            " 94% 150/160 [00:09<00:00, 15.54it/s]\u001b[A\n",
            " 95% 152/160 [00:09<00:00, 15.48it/s]\u001b[A\n",
            " 96% 154/160 [00:09<00:00, 15.54it/s]\u001b[A\n",
            " 98% 156/160 [00:10<00:00, 15.49it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.37758979201316833, 'eval_accuracy': 0.9638932496075353, 'eval_runtime': 10.3362, 'eval_samples_per_second': 123.256, 'eval_steps_per_second': 15.48, 'epoch': 1.0}\n",
            "100% 625/625 [02:29<00:00,  4.60it/s]\n",
            "100% 160/160 [00:10<00:00, 15.55it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2939] 2024-01-14 20:07:22,276 >> Saving model checkpoint to out/emotion/gpt2_simple/checkpoint-625\n",
            "[INFO|configuration_utils.py:460] 2024-01-14 20:07:22,277 >> Configuration saved in out/emotion/gpt2_simple/checkpoint-625/config.json\n",
            "[INFO|modeling_utils.py:2118] 2024-01-14 20:07:31,750 >> Model weights saved in out/emotion/gpt2_simple/checkpoint-625/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2024-01-14 20:07:31,750 >> tokenizer config file saved in out/emotion/gpt2_simple/checkpoint-625/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2024-01-14 20:07:31,751 >> Special tokens file saved in out/emotion/gpt2_simple/checkpoint-625/special_tokens_map.json\n",
            "[INFO|trainer.py:2017] 2024-01-14 20:07:43,282 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2196] 2024-01-14 20:07:43,282 >> Loading best model from out/emotion/gpt2_simple/checkpoint-625 (score: 0.9638932496075353).\n",
            "{'train_runtime': 171.1186, 'train_samples_per_second': 29.214, 'train_steps_per_second': 3.652, 'train_loss': 1.2380104187011718, 'epoch': 1.0}\n",
            "100% 625/625 [02:51<00:00,  3.65it/s]\n",
            "[INFO|trainer.py:2939] 2024-01-14 20:07:43,650 >> Saving model checkpoint to out/emotion/gpt2_simple\n",
            "[INFO|configuration_utils.py:460] 2024-01-14 20:07:43,651 >> Configuration saved in out/emotion/gpt2_simple/config.json\n",
            "[INFO|modeling_utils.py:2118] 2024-01-14 20:07:52,624 >> Model weights saved in out/emotion/gpt2_simple/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2024-01-14 20:07:52,626 >> tokenizer config file saved in out/emotion/gpt2_simple/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2024-01-14 20:07:52,626 >> Special tokens file saved in out/emotion/gpt2_simple/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =      1.238\n",
            "  train_runtime            = 0:02:51.11\n",
            "  train_samples            =       4999\n",
            "  train_samples_per_second =     29.214\n",
            "  train_steps_per_second   =      3.652\n",
            "01/14/2024 20:07:52 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:761] 2024-01-14 20:07:52,703 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassificationCustomSimple.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassificationCustomSimple.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3213] 2024-01-14 20:07:52,705 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2024-01-14 20:07:52,706 >>   Num examples = 1274\n",
            "[INFO|trainer.py:3218] 2024-01-14 20:07:52,706 >>   Batch size = 8\n",
            "100% 160/160 [00:10<00:00, 15.96it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.9639\n",
            "  eval_loss               =     0.3776\n",
            "  eval_runtime            = 0:00:10.12\n",
            "  eval_samples            =       1274\n",
            "  eval_samples_per_second =    125.837\n",
            "  eval_steps_per_second   =     15.804\n",
            "[INFO|modelcard.py:452] 2024-01-14 20:08:03,420 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9638932496075353}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_glue.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path roberta-base \\\n",
        "  --custom_model roberta_hidden_v2 \\\n",
        "  --train_file data/train-5k.json  \\\n",
        "  --validation_file data/valid-5k.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 1000 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 50 \\\n",
        "  --eval_steps 1000 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model 'accuracy' \\\n",
        "  --greater_is_better 'True' \\\n",
        "  --load_best_model_at_end 'True' \\\n",
        "  --report_to 'none' \\\n",
        "  --output_dir out/emotion/roberta_hidden_v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjpxvTXYsAqX",
        "outputId": "24cf3428-c759-4014-bc53-4afa56374204"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-14 20:08:35.360282: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-14 20:08:35.360337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-14 20:08:35.361682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-14 20:08:36.481476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/14/2024 20:08:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/14/2024 20:08:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/emotion/roberta_hidden_v2/runs/Jan14_20-08-38_35868d6163fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=50,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=out/emotion/roberta_hidden_v2,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/emotion/roberta_hidden_v2,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "01/14/2024 20:08:38 - INFO - __main__ - load a local file for train: data/train-5k.json\n",
            "01/14/2024 20:08:38 - INFO - __main__ - load a local file for validation: data/valid-5k.json\n",
            "Using custom data configuration default-b1c162b574f6f092\n",
            "01/14/2024 20:08:39 - INFO - datasets.builder - Using custom data configuration default-b1c162b574f6f092\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/14/2024 20:08:39 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "01/14/2024 20:08:39 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from .cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "01/14/2024 20:08:39 - INFO - datasets.info - Loading Dataset info from .cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "01/14/2024 20:08:39 - INFO - datasets.builder - Found cached dataset json (/content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "01/14/2024 20:08:39 - INFO - datasets.info - Loading Dataset info from /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Downloading config.json: 100% 481/481 [00:00<00:00, 2.11MB/s]\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:08:39,728 >> loading configuration file config.json from cache at .cache_training/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:08:39,729 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:550] 2024-01-14 20:08:39,834 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:08:39,939 >> loading configuration file config.json from cache at .cache_training/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:08:39,940 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Downloading vocab.json: 100% 899k/899k [00:00<00:00, 2.77MB/s]\n",
            "Downloading merges.txt: 100% 456k/456k [00:00<00:00, 1.84MB/s]\n",
            "Downloading tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.31MB/s]\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:08:42,039 >> loading file vocab.json from cache at .cache_training/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:08:42,039 >> loading file merges.txt from cache at .cache_training/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:08:42,039 >> loading file tokenizer.json from cache at .cache_training/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:08:42,039 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:08:42,039 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:08:42,039 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:08:42,039 >> loading configuration file config.json from cache at .cache_training/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:08:42,040 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "01/14/2024 20:08:42 - INFO - __main__ - Using hidden states in model: True\n",
            "01/14/2024 20:08:42 - INFO - __main__ - Using implementation from class: RobertaForSequenceClassificationCustomAlternative\n",
            "Downloading model.safetensors: 100% 499M/499M [00:01<00:00, 265MB/s]\n",
            "[INFO|modeling_utils.py:2993] 2024-01-14 20:08:44,228 >> loading weights file model.safetensors from cache at .cache_training/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors\n",
            "[INFO|modeling_utils.py:3765] 2024-01-14 20:08:46,601 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassificationCustomAlternative: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassificationCustomAlternative from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassificationCustomAlternative from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3777] 2024-01-14 20:08:46,601 >> Some weights of RobertaForSequenceClassificationCustomAlternative were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense_1_input.bias', 'classifier.dense_1_hidden.bias', 'classifier.dense_2.weight', 'classifier.out_proj.bias', 'classifier.dense_2.bias', 'classifier.out_proj.weight', 'classifier.dense_1_input.weight', 'classifier.dense_1_hidden.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Model: $RobertaForSequenceClassificationCustomAlternative(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): RobertaClassificationHeadCustomAlternative(\n",
            "    (dense_1_input): Linear(in_features=768, out_features=1536, bias=True)\n",
            "    (dense_1_hidden): Linear(in_features=768, out_features=1536, bias=True)\n",
            "    (dense_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "Running tokenizer on dataset:   0% 0/4999 [00:00<?, ? examples/s]Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c241b11d76605b85.arrow\n",
            "01/14/2024 20:08:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c241b11d76605b85.arrow\n",
            "Running tokenizer on dataset: 100% 4999/4999 [00:00<00:00, 8952.50 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1274 [00:00<?, ? examples/s]Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-330134beeb5326e3.arrow\n",
            "01/14/2024 20:08:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/.cache_training/json/default-b1c162b574f6f092/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-330134beeb5326e3.arrow\n",
            "Running tokenizer on dataset: 100% 1274/1274 [00:00<00:00, 9571.63 examples/s]\n",
            "01/14/2024 20:08:47 - INFO - __main__ - Sample 912 of the training set: {'label': 2, 'text': 'i feel we need a little romantic boost in the relationship', 'input_ids': [0, 118, 619, 52, 240, 10, 410, 8728, 2501, 11, 5, 1291, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/14/2024 20:08:47 - INFO - __main__ - Sample 204 of the training set: {'label': 1, 'text': 'i feel pretty mellow so far about whatever healing wounding process may be getting underway', 'input_ids': [0, 118, 619, 1256, 34384, 1722, 98, 444, 59, 3046, 11759, 21354, 609, 189, 28, 562, 6159, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/14/2024 20:08:47 - INFO - __main__ - Sample 2253 of the training set: {'label': 1, 'text': 'i feel ive answered those questions for her and shes pretty trusting for the most part', 'input_ids': [0, 118, 619, 1437, 2088, 7173, 167, 1142, 13, 69, 8, 79, 29, 1256, 28969, 13, 5, 144, 233, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:761] 2024-01-14 20:08:48,693 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassificationCustomAlternative.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustomAlternative.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1760] 2024-01-14 20:08:48,702 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2024-01-14 20:08:48,702 >>   Num examples = 4,999\n",
            "[INFO|trainer.py:1762] 2024-01-14 20:08:48,702 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1763] 2024-01-14 20:08:48,702 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1766] 2024-01-14 20:08:48,702 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1767] 2024-01-14 20:08:48,702 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1768] 2024-01-14 20:08:48,702 >>   Total optimization steps = 625\n",
            "[INFO|trainer.py:1769] 2024-01-14 20:08:48,703 >>   Number of trainable parameters = 128,782,086\n",
            "{'loss': 1.6138, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.08}\n",
            "{'loss': 1.3319, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.16}\n",
            "{'loss': 1.2611, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.24}\n",
            "{'loss': 1.0844, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.32}\n",
            "{'loss': 0.9535, 'learning_rate': 1.2e-05, 'epoch': 0.4}\n",
            "{'loss': 0.8535, 'learning_rate': 1.04e-05, 'epoch': 0.48}\n",
            "{'loss': 0.705, 'learning_rate': 8.8e-06, 'epoch': 0.56}\n",
            "{'loss': 0.7042, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.64}\n",
            "{'loss': 0.6446, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.72}\n",
            "{'loss': 0.6465, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
            "{'loss': 0.5608, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.88}\n",
            "{'loss': 0.5601, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.96}\n",
            "{'loss': 0.6953, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 625/625 [02:06<00:00,  4.99it/s][INFO|trainer.py:761] 2024-01-14 20:10:55,093 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustomAlternative.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustomAlternative.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3213] 2024-01-14 20:10:55,094 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2024-01-14 20:10:55,095 >>   Num examples = 1274\n",
            "[INFO|trainer.py:3218] 2024-01-14 20:10:55,095 >>   Batch size = 8\n",
            "\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/160 [00:00<00:05, 27.06it/s]\u001b[A\n",
            "  4% 6/160 [00:00<00:07, 20.85it/s]\u001b[A\n",
            "  6% 9/160 [00:00<00:07, 19.46it/s]\u001b[A\n",
            "  8% 12/160 [00:00<00:07, 18.91it/s]\u001b[A\n",
            "  9% 14/160 [00:00<00:07, 18.69it/s]\u001b[A\n",
            " 10% 16/160 [00:00<00:07, 18.41it/s]\u001b[A\n",
            " 11% 18/160 [00:00<00:07, 18.33it/s]\u001b[A\n",
            " 12% 20/160 [00:01<00:07, 18.27it/s]\u001b[A\n",
            " 14% 22/160 [00:01<00:07, 18.21it/s]\u001b[A\n",
            " 15% 24/160 [00:01<00:07, 18.03it/s]\u001b[A\n",
            " 16% 26/160 [00:01<00:07, 18.05it/s]\u001b[A\n",
            " 18% 28/160 [00:01<00:07, 18.01it/s]\u001b[A\n",
            " 19% 30/160 [00:01<00:07, 18.01it/s]\u001b[A\n",
            " 20% 32/160 [00:01<00:07, 17.94it/s]\u001b[A\n",
            " 21% 34/160 [00:01<00:06, 18.03it/s]\u001b[A\n",
            " 22% 36/160 [00:01<00:06, 17.97it/s]\u001b[A\n",
            " 24% 38/160 [00:02<00:06, 18.00it/s]\u001b[A\n",
            " 25% 40/160 [00:02<00:06, 17.98it/s]\u001b[A\n",
            " 26% 42/160 [00:02<00:06, 18.01it/s]\u001b[A\n",
            " 28% 44/160 [00:02<00:06, 17.98it/s]\u001b[A\n",
            " 29% 46/160 [00:02<00:06, 18.05it/s]\u001b[A\n",
            " 30% 48/160 [00:02<00:06, 18.03it/s]\u001b[A\n",
            " 31% 50/160 [00:02<00:06, 18.13it/s]\u001b[A\n",
            " 32% 52/160 [00:02<00:05, 18.06it/s]\u001b[A\n",
            " 34% 54/160 [00:02<00:05, 18.11it/s]\u001b[A\n",
            " 35% 56/160 [00:03<00:05, 18.08it/s]\u001b[A\n",
            " 36% 58/160 [00:03<00:05, 18.09it/s]\u001b[A\n",
            " 38% 60/160 [00:03<00:05, 18.05it/s]\u001b[A\n",
            " 39% 62/160 [00:03<00:05, 18.11it/s]\u001b[A\n",
            " 40% 64/160 [00:03<00:05, 18.06it/s]\u001b[A\n",
            " 41% 66/160 [00:03<00:05, 18.00it/s]\u001b[A\n",
            " 42% 68/160 [00:03<00:05, 18.06it/s]\u001b[A\n",
            " 44% 70/160 [00:03<00:04, 18.06it/s]\u001b[A\n",
            " 45% 72/160 [00:03<00:04, 18.06it/s]\u001b[A\n",
            " 46% 74/160 [00:04<00:04, 18.04it/s]\u001b[A\n",
            " 48% 76/160 [00:04<00:04, 18.06it/s]\u001b[A\n",
            " 49% 78/160 [00:04<00:04, 17.97it/s]\u001b[A\n",
            " 50% 80/160 [00:04<00:04, 18.07it/s]\u001b[A\n",
            " 51% 82/160 [00:04<00:04, 17.98it/s]\u001b[A\n",
            " 52% 84/160 [00:04<00:04, 17.97it/s]\u001b[A\n",
            " 54% 86/160 [00:04<00:04, 18.02it/s]\u001b[A\n",
            " 55% 88/160 [00:04<00:03, 18.09it/s]\u001b[A\n",
            " 56% 90/160 [00:04<00:03, 18.03it/s]\u001b[A\n",
            " 57% 92/160 [00:05<00:03, 18.07it/s]\u001b[A\n",
            " 59% 94/160 [00:05<00:03, 18.02it/s]\u001b[A\n",
            " 60% 96/160 [00:05<00:03, 18.03it/s]\u001b[A\n",
            " 61% 98/160 [00:05<00:03, 17.98it/s]\u001b[A\n",
            " 62% 100/160 [00:05<00:03, 18.01it/s]\u001b[A\n",
            " 64% 102/160 [00:05<00:03, 17.91it/s]\u001b[A\n",
            " 65% 104/160 [00:05<00:03, 17.93it/s]\u001b[A\n",
            " 66% 106/160 [00:05<00:03, 17.91it/s]\u001b[A\n",
            " 68% 108/160 [00:05<00:02, 18.01it/s]\u001b[A\n",
            " 69% 110/160 [00:06<00:02, 18.00it/s]\u001b[A\n",
            " 70% 112/160 [00:06<00:02, 18.03it/s]\u001b[A\n",
            " 71% 114/160 [00:06<00:02, 18.00it/s]\u001b[A\n",
            " 72% 116/160 [00:06<00:02, 18.02it/s]\u001b[A\n",
            " 74% 118/160 [00:06<00:02, 17.99it/s]\u001b[A\n",
            " 75% 120/160 [00:06<00:02, 17.99it/s]\u001b[A\n",
            " 76% 122/160 [00:06<00:02, 17.92it/s]\u001b[A\n",
            " 78% 124/160 [00:06<00:02, 17.88it/s]\u001b[A\n",
            " 79% 126/160 [00:06<00:01, 17.88it/s]\u001b[A\n",
            " 80% 128/160 [00:07<00:01, 17.81it/s]\u001b[A\n",
            " 81% 130/160 [00:07<00:01, 17.92it/s]\u001b[A\n",
            " 82% 132/160 [00:07<00:01, 17.78it/s]\u001b[A\n",
            " 84% 134/160 [00:07<00:01, 17.86it/s]\u001b[A\n",
            " 85% 136/160 [00:07<00:01, 17.78it/s]\u001b[A\n",
            " 86% 138/160 [00:07<00:01, 17.86it/s]\u001b[A\n",
            " 88% 140/160 [00:07<00:01, 17.74it/s]\u001b[A\n",
            " 89% 142/160 [00:07<00:01, 17.74it/s]\u001b[A\n",
            " 90% 144/160 [00:07<00:00, 17.74it/s]\u001b[A\n",
            " 91% 146/160 [00:08<00:00, 17.65it/s]\u001b[A\n",
            " 92% 148/160 [00:08<00:00, 17.74it/s]\u001b[A\n",
            " 94% 150/160 [00:08<00:00, 17.62it/s]\u001b[A\n",
            " 95% 152/160 [00:08<00:00, 17.58it/s]\u001b[A\n",
            " 96% 154/160 [00:08<00:00, 17.68it/s]\u001b[A\n",
            " 98% 156/160 [00:08<00:00, 17.68it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.2501935064792633, 'eval_accuracy': 0.9246467817896389, 'eval_runtime': 8.8935, 'eval_samples_per_second': 143.251, 'eval_steps_per_second': 17.991, 'epoch': 1.0}\n",
            "100% 625/625 [02:15<00:00,  4.99it/s]\n",
            "100% 160/160 [00:08<00:00, 17.78it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2939] 2024-01-14 20:11:03,989 >> Saving model checkpoint to out/emotion/roberta_hidden_v2/checkpoint-625\n",
            "[INFO|configuration_utils.py:460] 2024-01-14 20:11:03,991 >> Configuration saved in out/emotion/roberta_hidden_v2/checkpoint-625/config.json\n",
            "[INFO|modeling_utils.py:2118] 2024-01-14 20:11:05,663 >> Model weights saved in out/emotion/roberta_hidden_v2/checkpoint-625/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2024-01-14 20:11:05,665 >> tokenizer config file saved in out/emotion/roberta_hidden_v2/checkpoint-625/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2024-01-14 20:11:05,665 >> Special tokens file saved in out/emotion/roberta_hidden_v2/checkpoint-625/special_tokens_map.json\n",
            "[INFO|trainer.py:2017] 2024-01-14 20:11:09,908 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2196] 2024-01-14 20:11:09,909 >> Loading best model from out/emotion/roberta_hidden_v2/checkpoint-625 (score: 0.9246467817896389).\n",
            "{'train_runtime': 141.6822, 'train_samples_per_second': 35.283, 'train_steps_per_second': 4.411, 'train_loss': 0.9013549468994141, 'epoch': 1.0}\n",
            "100% 625/625 [02:21<00:00,  4.41it/s]\n",
            "[INFO|trainer.py:2939] 2024-01-14 20:11:10,386 >> Saving model checkpoint to out/emotion/roberta_hidden_v2\n",
            "[INFO|configuration_utils.py:460] 2024-01-14 20:11:10,388 >> Configuration saved in out/emotion/roberta_hidden_v2/config.json\n",
            "[INFO|modeling_utils.py:2118] 2024-01-14 20:11:12,045 >> Model weights saved in out/emotion/roberta_hidden_v2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2024-01-14 20:11:12,047 >> tokenizer config file saved in out/emotion/roberta_hidden_v2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2024-01-14 20:11:12,047 >> Special tokens file saved in out/emotion/roberta_hidden_v2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     0.9014\n",
            "  train_runtime            = 0:02:21.68\n",
            "  train_samples            =       4999\n",
            "  train_samples_per_second =     35.283\n",
            "  train_steps_per_second   =      4.411\n",
            "01/14/2024 20:11:12 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:761] 2024-01-14 20:11:12,102 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassificationCustomAlternative.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassificationCustomAlternative.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3213] 2024-01-14 20:11:12,103 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2024-01-14 20:11:12,104 >>   Num examples = 1274\n",
            "[INFO|trainer.py:3218] 2024-01-14 20:11:12,104 >>   Batch size = 8\n",
            "100% 160/160 [00:08<00:00, 18.15it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.9246\n",
            "  eval_loss               =     0.2502\n",
            "  eval_runtime            = 0:00:08.90\n",
            "  eval_samples            =       1274\n",
            "  eval_samples_per_second =    143.115\n",
            "  eval_steps_per_second   =     17.974\n",
            "[INFO|modelcard.py:452] 2024-01-14 20:11:21,177 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9246467817896389}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_translation.py \\\n",
        "  --cache_dir .cache_training \\\n",
        "  --model_name_or_path \"google/t5-v1_1-small\" \\\n",
        "  --train_file data/s2s-train.json \\\n",
        "  --validation_file data/s2s-valid-5k.json \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --source_lang \"text\" \\\n",
        "  --target_lang \"label\" \\\n",
        "  --source_prefix \"emotion classification\" \\\n",
        "  --max_source_length 256 \\\n",
        "  --max_target_length 128 \\\n",
        "  --generation_max_length 128 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --predict_with_generate \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 1000 \\\n",
        "  --save_total_limit 5 \\\n",
        "  --logging_strategy steps \\\n",
        "  --logging_steps 50 \\\n",
        "  --eval_steps 1000 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --metric_for_best_model 'accuracy' \\\n",
        "  --greater_is_better 'True' \\\n",
        "  --load_best_model_at_end 'True' \\\n",
        "  --report_to 'none' \\\n",
        "  --output_dir out/emotion/t5_v1_1"
      ],
      "metadata": {
        "id": "lYo98oFvDKf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161e9b49-d42c-493b-9d55-d58ed61a1b17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-14 20:11:34.358644: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-14 20:11:34.358701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-14 20:11:34.360070: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-14 20:11:35.508296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/14/2024 20:11:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/14/2024 20:11:38 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=128,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=out/emotion/t5_v1_1/runs/Jan14_20-11-37_35868d6163fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=50,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=accuracy,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=out/emotion/t5_v1_1,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=out/emotion/t5_v1_1,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=5,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-7228be41958265fb\n",
            "01/14/2024 20:11:38 - INFO - datasets.builder - Using custom data configuration default-7228be41958265fb\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/14/2024 20:11:38 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "01/14/2024 20:11:38 - INFO - datasets.builder - Generating dataset json (/content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "01/14/2024 20:11:38 - INFO - datasets.builder - Downloading and preparing dataset json/default to /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 12614.45it/s]\n",
            "Downloading took 0.0 min\n",
            "01/14/2024 20:11:38 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "01/14/2024 20:11:38 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 182.71it/s]\n",
            "Generating train split\n",
            "01/14/2024 20:11:38 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 16000 examples [00:00, 495198.93 examples/s]\n",
            "Generating validation split\n",
            "01/14/2024 20:11:38 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 1274 examples [00:00, 503205.89 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "01/14/2024 20:11:38 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "01/14/2024 20:11:38 - INFO - datasets.builder - Dataset json downloaded and prepared to /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Downloading config.json: 100% 537/537 [00:00<00:00, 2.77MB/s]\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:11:38,911 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:11:38,917 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading tokenizer_config.json: 100% 1.86k/1.86k [00:00<00:00, 10.3MB/s]\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:11:39,130 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:11:39,131 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 2.44MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 1.79k/1.79k [00:00<00:00, 8.69MB/s]\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:11:40,083 >> loading file spiece.model from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/spiece.model\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:11:40,083 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:11:40,083 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:11:40,083 >> loading file special_tokens_map.json from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2024-01-14 20:11:40,083 >> loading file tokenizer_config.json from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:11:40,083 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:11:40,084 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:305] 2024-01-14 20:11:40,149 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "[INFO|configuration_utils.py:715] 2024-01-14 20:11:40,315 >> loading configuration file config.json from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/config.json\n",
            "[INFO|configuration_utils.py:775] 2024-01-14 20:11:40,316 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"google/t5-v1_1-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 308M/308M [00:15<00:00, 20.2MB/s]\n",
            "[INFO|modeling_utils.py:2993] 2024-01-14 20:11:56,854 >> loading weights file pytorch_model.bin from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:770] 2024-01-14 20:11:57,076 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3775] 2024-01-14 20:11:58,210 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:3783] 2024-01-14 20:11:58,210 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/t5-v1_1-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Downloading generation_config.json: 100% 147/147 [00:00<00:00, 830kB/s]\n",
            "[INFO|configuration_utils.py:730] 2024-01-14 20:11:58,427 >> loading configuration file generation_config.json from cache at .cache_training/models--google--t5-v1_1-small/snapshots/8a88af75516269158a3aa488d1abdfd3d5e4ee49/generation_config.json\n",
            "[INFO|configuration_utils.py:770] 2024-01-14 20:11:58,427 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "01/14/2024 20:11:58 - INFO - __main__ - Using translation prefix: \"'emotion classification: '\"\n",
            "Running tokenizer on train dataset:   0% 0/16000 [00:00<?, ? examples/s]Caching processed dataset at /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-396b6c3f1e0994d3.arrow\n",
            "01/14/2024 20:11:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-396b6c3f1e0994d3.arrow\n",
            "Running tokenizer on train dataset: 100% 16000/16000 [00:01<00:00, 12022.02 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1274 [00:00<?, ? examples/s]Caching processed dataset at /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839c5df52c71250f.arrow\n",
            "01/14/2024 20:11:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/.cache_training/json/default-7228be41958265fb/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-839c5df52c71250f.arrow\n",
            "Running tokenizer on validation dataset: 100% 1274/1274 [00:00<00:00, 12356.38 examples/s]\n",
            "Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 23.2MB/s]\n",
            "[INFO|trainer.py:1760] 2024-01-14 20:12:02,409 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2024-01-14 20:12:02,409 >>   Num examples = 16,000\n",
            "[INFO|trainer.py:1762] 2024-01-14 20:12:02,409 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1763] 2024-01-14 20:12:02,409 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1766] 2024-01-14 20:12:02,409 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1767] 2024-01-14 20:12:02,409 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1768] 2024-01-14 20:12:02,409 >>   Total optimization steps = 2,000\n",
            "[INFO|trainer.py:1769] 2024-01-14 20:12:02,410 >>   Number of trainable parameters = 76,961,152\n",
            "  0% 0/2000 [00:00<?, ?it/s][WARNING|logging.py:290] 2024-01-14 20:12:02,413 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 22.615, 'learning_rate': 4.875e-05, 'epoch': 0.03}\n",
            "{'loss': 19.9169, 'learning_rate': 4.75e-05, 'epoch': 0.05}\n",
            "{'loss': 16.1726, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.07}\n",
            "{'loss': 13.6059, 'learning_rate': 4.5e-05, 'epoch': 0.1}\n",
            "{'loss': 10.7349, 'learning_rate': 4.375e-05, 'epoch': 0.12}\n",
            "{'loss': 8.1823, 'learning_rate': 4.25e-05, 'epoch': 0.15}\n",
            "{'loss': 5.3253, 'learning_rate': 4.125e-05, 'epoch': 0.17}\n",
            "{'loss': 3.3077, 'learning_rate': 4e-05, 'epoch': 0.2}\n",
            "{'loss': 2.4757, 'learning_rate': 3.875e-05, 'epoch': 0.23}\n",
            "{'loss': 2.3823, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.25}\n",
            "{'loss': 1.8477, 'learning_rate': 3.625e-05, 'epoch': 0.28}\n",
            "{'loss': 1.7609, 'learning_rate': 3.5e-05, 'epoch': 0.3}\n",
            "{'loss': 1.4104, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.33}\n",
            "{'loss': 1.4268, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.35}\n",
            "{'loss': 1.2956, 'learning_rate': 3.125e-05, 'epoch': 0.38}\n",
            "{'loss': 1.3243, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
            "{'loss': 1.2151, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.42}\n",
            "{'loss': 1.1816, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.45}\n",
            "{'loss': 1.1237, 'learning_rate': 2.625e-05, 'epoch': 0.47}\n",
            "{'loss': 1.0823, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n",
            " 50% 1000/2000 [01:43<01:34, 10.55it/s][INFO|trainer.py:3213] 2024-01-14 20:13:46,179 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2024-01-14 20:13:46,179 >>   Num examples = 1274\n",
            "[INFO|trainer.py:3218] 2024-01-14 20:13:46,179 >>   Batch size = 8\n",
            "[INFO|configuration_utils.py:770] 2024-01-14 20:13:46,183 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/160 [00:00<00:07, 19.63it/s]\u001b[A\n",
            "  3% 5/160 [00:00<00:10, 15.47it/s]\u001b[A\n",
            "  4% 7/160 [00:00<00:10, 14.60it/s]\u001b[A\n",
            "  6% 9/160 [00:00<00:10, 13.82it/s]\u001b[A\n",
            "  7% 11/160 [00:00<00:10, 13.81it/s]\u001b[A\n",
            "  8% 13/160 [00:00<00:10, 13.81it/s]\u001b[A\n",
            "  9% 15/160 [00:01<00:10, 13.41it/s]\u001b[A\n",
            " 11% 17/160 [00:01<00:10, 13.01it/s]\u001b[A\n",
            " 12% 19/160 [00:01<00:10, 13.06it/s]\u001b[A\n",
            " 13% 21/160 [00:01<00:10, 12.84it/s]\u001b[A\n",
            " 14% 23/160 [00:01<00:10, 12.79it/s]\u001b[A\n",
            " 16% 25/160 [00:01<00:09, 14.06it/s]\u001b[A\n",
            " 17% 27/160 [00:01<00:08, 15.03it/s]\u001b[A\n",
            " 18% 29/160 [00:02<00:08, 15.53it/s]\u001b[A\n",
            " 19% 31/160 [00:02<00:08, 15.63it/s]\u001b[A\n",
            " 21% 33/160 [00:02<00:08, 15.75it/s]\u001b[A\n",
            " 22% 35/160 [00:02<00:07, 15.98it/s]\u001b[A\n",
            " 23% 37/160 [00:02<00:07, 15.91it/s]\u001b[A\n",
            " 24% 39/160 [00:02<00:07, 16.35it/s]\u001b[A\n",
            " 26% 41/160 [00:02<00:07, 16.51it/s]\u001b[A\n",
            " 27% 43/160 [00:02<00:06, 16.88it/s]\u001b[A\n",
            " 28% 45/160 [00:03<00:06, 16.84it/s]\u001b[A\n",
            " 29% 47/160 [00:03<00:06, 16.54it/s]\u001b[A\n",
            " 31% 49/160 [00:03<00:06, 16.96it/s]\u001b[A\n",
            " 32% 51/160 [00:03<00:06, 17.19it/s]\u001b[A\n",
            " 33% 53/160 [00:03<00:06, 17.50it/s]\u001b[A\n",
            " 34% 55/160 [00:03<00:05, 17.69it/s]\u001b[A\n",
            " 36% 57/160 [00:03<00:05, 17.49it/s]\u001b[A\n",
            " 37% 59/160 [00:03<00:05, 17.69it/s]\u001b[A\n",
            " 38% 61/160 [00:03<00:05, 17.61it/s]\u001b[A\n",
            " 39% 63/160 [00:04<00:05, 17.71it/s]\u001b[A\n",
            " 41% 65/160 [00:04<00:05, 16.97it/s]\u001b[A\n",
            " 42% 67/160 [00:04<00:05, 17.10it/s]\u001b[A\n",
            " 43% 69/160 [00:04<00:05, 17.36it/s]\u001b[A\n",
            " 44% 71/160 [00:04<00:05, 17.21it/s]\u001b[A\n",
            " 46% 73/160 [00:04<00:05, 17.27it/s]\u001b[A\n",
            " 47% 75/160 [00:04<00:04, 17.33it/s]\u001b[A\n",
            " 48% 77/160 [00:04<00:04, 17.39it/s]\u001b[A\n",
            " 49% 79/160 [00:04<00:04, 17.56it/s]\u001b[A\n",
            " 51% 81/160 [00:05<00:04, 17.82it/s]\u001b[A\n",
            " 52% 83/160 [00:05<00:04, 17.32it/s]\u001b[A\n",
            " 53% 85/160 [00:05<00:04, 17.54it/s]\u001b[A\n",
            " 54% 87/160 [00:05<00:04, 17.68it/s]\u001b[A\n",
            " 56% 89/160 [00:05<00:04, 17.38it/s]\u001b[A\n",
            " 57% 91/160 [00:05<00:03, 17.58it/s]\u001b[A\n",
            " 58% 93/160 [00:05<00:03, 17.85it/s]\u001b[A\n",
            " 59% 95/160 [00:05<00:03, 17.51it/s]\u001b[A\n",
            " 61% 97/160 [00:05<00:03, 17.75it/s]\u001b[A\n",
            " 62% 99/160 [00:06<00:03, 17.97it/s]\u001b[A\n",
            " 63% 101/160 [00:06<00:03, 17.28it/s]\u001b[A\n",
            " 64% 103/160 [00:06<00:03, 17.62it/s]\u001b[A\n",
            " 66% 105/160 [00:06<00:03, 17.88it/s]\u001b[A\n",
            " 67% 107/160 [00:06<00:03, 17.52it/s]\u001b[A\n",
            " 68% 109/160 [00:06<00:02, 17.58it/s]\u001b[A\n",
            " 69% 111/160 [00:06<00:02, 17.74it/s]\u001b[A\n",
            " 71% 113/160 [00:06<00:02, 17.61it/s]\u001b[A\n",
            " 72% 115/160 [00:07<00:02, 17.41it/s]\u001b[A\n",
            " 73% 117/160 [00:07<00:02, 17.38it/s]\u001b[A\n",
            " 74% 119/160 [00:07<00:02, 16.91it/s]\u001b[A\n",
            " 76% 121/160 [00:07<00:02, 17.25it/s]\u001b[A\n",
            " 77% 123/160 [00:07<00:02, 17.26it/s]\u001b[A\n",
            " 78% 125/160 [00:07<00:02, 16.95it/s]\u001b[A\n",
            " 79% 127/160 [00:07<00:01, 16.61it/s]\u001b[A\n",
            " 81% 129/160 [00:07<00:01, 16.78it/s]\u001b[A\n",
            " 82% 131/160 [00:07<00:01, 16.95it/s]\u001b[A\n",
            " 83% 133/160 [00:08<00:01, 17.03it/s]\u001b[A\n",
            " 84% 135/160 [00:08<00:01, 16.61it/s]\u001b[A\n",
            " 86% 137/160 [00:08<00:01, 16.85it/s]\u001b[A\n",
            " 87% 139/160 [00:08<00:01, 16.73it/s]\u001b[A\n",
            " 88% 141/160 [00:08<00:01, 16.89it/s]\u001b[A\n",
            " 89% 143/160 [00:08<00:01, 16.85it/s]\u001b[A\n",
            " 91% 145/160 [00:08<00:00, 17.21it/s]\u001b[A\n",
            " 92% 147/160 [00:08<00:00, 17.18it/s]\u001b[A\n",
            " 93% 149/160 [00:09<00:00, 17.30it/s]\u001b[A\n",
            " 94% 151/160 [00:09<00:00, 17.47it/s]\u001b[A\n",
            " 96% 153/160 [00:09<00:00, 16.93it/s]\u001b[A\n",
            " 97% 155/160 [00:09<00:00, 17.15it/s]\u001b[A\n",
            " 98% 157/160 [00:09<00:00, 17.43it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.21813049912452698, 'eval_bleu': 0.0, 'eval_accuracy': 0.8791, 'eval_gen_len': 2.0, 'eval_runtime': 12.8555, 'eval_samples_per_second': 99.102, 'eval_steps_per_second': 12.446, 'epoch': 0.5}\n",
            " 50% 1000/2000 [01:56<01:34, 10.55it/s]\n",
            "100% 160/160 [00:10<00:00, 17.19it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2939] 2024-01-14 20:13:59,035 >> Saving model checkpoint to out/emotion/t5_v1_1/checkpoint-1000\n",
            "[INFO|configuration_utils.py:460] 2024-01-14 20:13:59,037 >> Configuration saved in out/emotion/t5_v1_1/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:544] 2024-01-14 20:13:59,037 >> Configuration saved in out/emotion/t5_v1_1/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:2118] 2024-01-14 20:13:59,775 >> Model weights saved in out/emotion/t5_v1_1/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2024-01-14 20:13:59,780 >> tokenizer config file saved in out/emotion/t5_v1_1/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2024-01-14 20:13:59,780 >> Special tokens file saved in out/emotion/t5_v1_1/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:189] 2024-01-14 20:13:59,781 >> Copy vocab file to out/emotion/t5_v1_1/checkpoint-1000/spiece.model\n",
            "{'loss': 1.0748, 'learning_rate': 2.375e-05, 'epoch': 0.53}\n",
            "{'loss': 0.9142, 'learning_rate': 2.25e-05, 'epoch': 0.55}\n",
            "{'loss': 0.9609, 'learning_rate': 2.125e-05, 'epoch': 0.57}\n",
            "{'loss': 0.8958, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
            "{'loss': 0.9645, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.62}\n",
            "{'loss': 0.9318, 'learning_rate': 1.75e-05, 'epoch': 0.65}\n",
            "{'loss': 0.9163, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.68}\n",
            "{'loss': 1.0114, 'learning_rate': 1.5e-05, 'epoch': 0.7}\n",
            "{'loss': 0.9142, 'learning_rate': 1.3750000000000002e-05, 'epoch': 0.72}\n",
            "{'loss': 0.8402, 'learning_rate': 1.25e-05, 'epoch': 0.75}\n",
            "{'loss': 0.8368, 'learning_rate': 1.125e-05, 'epoch': 0.78}\n",
            "{'loss': 0.8761, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
            "{'loss': 0.9364, 'learning_rate': 8.75e-06, 'epoch': 0.82}\n",
            "{'loss': 0.854, 'learning_rate': 7.5e-06, 'epoch': 0.85}\n",
            "{'loss': 0.8527, 'learning_rate': 6.25e-06, 'epoch': 0.88}\n",
            "{'loss': 0.8787, 'learning_rate': 5e-06, 'epoch': 0.9}\n",
            "{'loss': 0.9457, 'learning_rate': 3.75e-06, 'epoch': 0.93}\n",
            "{'loss': 0.812, 'learning_rate': 2.5e-06, 'epoch': 0.95}\n",
            "{'loss': 0.9693, 'learning_rate': 1.25e-06, 'epoch': 0.97}\n",
            "{'loss': 0.9698, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 2000/2000 [03:43<00:00,  8.24it/s][INFO|trainer.py:3213] 2024-01-14 20:15:45,713 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2024-01-14 20:15:45,714 >>   Num examples = 1274\n",
            "[INFO|trainer.py:3218] 2024-01-14 20:15:45,714 >>   Batch size = 8\n",
            "\n",
            "  0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 3/160 [00:00<00:08, 18.40it/s]\u001b[A\n",
            "  3% 5/160 [00:00<00:10, 14.61it/s]\u001b[A\n",
            "  4% 7/160 [00:00<00:11, 13.66it/s]\u001b[A\n",
            "  6% 9/160 [00:00<00:11, 12.82it/s]\u001b[A\n",
            "  7% 11/160 [00:00<00:11, 13.21it/s]\u001b[A\n",
            "  8% 13/160 [00:00<00:10, 14.02it/s]\u001b[A\n",
            "  9% 15/160 [00:01<00:09, 14.59it/s]\u001b[A\n",
            " 11% 17/160 [00:01<00:09, 15.23it/s]\u001b[A\n",
            " 12% 19/160 [00:01<00:09, 15.65it/s]\u001b[A\n",
            " 13% 21/160 [00:01<00:08, 16.20it/s]\u001b[A\n",
            " 14% 23/160 [00:01<00:08, 16.64it/s]\u001b[A\n",
            " 16% 25/160 [00:01<00:08, 16.74it/s]\u001b[A\n",
            " 17% 27/160 [00:01<00:07, 16.99it/s]\u001b[A\n",
            " 18% 29/160 [00:01<00:07, 17.38it/s]\u001b[A\n",
            " 19% 31/160 [00:01<00:07, 17.31it/s]\u001b[A\n",
            " 21% 33/160 [00:02<00:07, 17.05it/s]\u001b[A\n",
            " 22% 35/160 [00:02<00:07, 17.04it/s]\u001b[A\n",
            " 23% 37/160 [00:02<00:07, 17.24it/s]\u001b[A\n",
            " 24% 39/160 [00:02<00:06, 17.38it/s]\u001b[A\n",
            " 26% 41/160 [00:02<00:06, 17.34it/s]\u001b[A\n",
            " 27% 43/160 [00:02<00:06, 17.50it/s]\u001b[A\n",
            " 28% 45/160 [00:02<00:06, 17.50it/s]\u001b[A\n",
            " 29% 47/160 [00:02<00:06, 17.50it/s]\u001b[A\n",
            " 31% 49/160 [00:03<00:06, 17.62it/s]\u001b[A\n",
            " 32% 51/160 [00:03<00:06, 17.04it/s]\u001b[A\n",
            " 33% 53/160 [00:03<00:06, 17.29it/s]\u001b[A\n",
            " 34% 55/160 [00:03<00:05, 17.63it/s]\u001b[A\n",
            " 36% 57/160 [00:03<00:05, 17.62it/s]\u001b[A\n",
            " 37% 59/160 [00:03<00:05, 17.37it/s]\u001b[A\n",
            " 38% 61/160 [00:03<00:05, 17.59it/s]\u001b[A\n",
            " 39% 63/160 [00:03<00:05, 17.34it/s]\u001b[A\n",
            " 41% 65/160 [00:03<00:05, 17.42it/s]\u001b[A\n",
            " 42% 67/160 [00:04<00:05, 17.52it/s]\u001b[A\n",
            " 43% 69/160 [00:04<00:05, 16.84it/s]\u001b[A\n",
            " 44% 71/160 [00:04<00:05, 17.09it/s]\u001b[A\n",
            " 46% 73/160 [00:04<00:05, 17.13it/s]\u001b[A\n",
            " 47% 75/160 [00:04<00:04, 17.26it/s]\u001b[A\n",
            " 48% 77/160 [00:04<00:04, 17.26it/s]\u001b[A\n",
            " 49% 79/160 [00:04<00:04, 17.19it/s]\u001b[A\n",
            " 51% 81/160 [00:04<00:04, 16.81it/s]\u001b[A\n",
            " 52% 83/160 [00:05<00:04, 16.51it/s]\u001b[A\n",
            " 53% 85/160 [00:05<00:04, 16.18it/s]\u001b[A\n",
            " 54% 87/160 [00:05<00:04, 16.63it/s]\u001b[A\n",
            " 56% 89/160 [00:05<00:04, 16.54it/s]\u001b[A\n",
            " 57% 91/160 [00:05<00:04, 16.21it/s]\u001b[A\n",
            " 58% 93/160 [00:05<00:04, 16.24it/s]\u001b[A\n",
            " 59% 95/160 [00:05<00:03, 16.45it/s]\u001b[A\n",
            " 61% 97/160 [00:05<00:03, 15.92it/s]\u001b[A\n",
            " 62% 99/160 [00:05<00:03, 16.26it/s]\u001b[A\n",
            " 63% 101/160 [00:06<00:03, 16.62it/s]\u001b[A\n",
            " 64% 103/160 [00:06<00:03, 16.32it/s]\u001b[A\n",
            " 66% 105/160 [00:06<00:03, 16.37it/s]\u001b[A\n",
            " 67% 107/160 [00:06<00:03, 16.50it/s]\u001b[A\n",
            " 68% 109/160 [00:06<00:03, 16.60it/s]\u001b[A\n",
            " 69% 111/160 [00:06<00:02, 16.84it/s]\u001b[A\n",
            " 71% 113/160 [00:06<00:02, 16.87it/s]\u001b[A\n",
            " 72% 115/160 [00:06<00:02, 17.22it/s]\u001b[A\n",
            " 73% 117/160 [00:07<00:02, 17.51it/s]\u001b[A\n",
            " 74% 119/160 [00:07<00:02, 16.94it/s]\u001b[A\n",
            " 76% 121/160 [00:07<00:02, 17.13it/s]\u001b[A\n",
            " 77% 123/160 [00:07<00:02, 17.02it/s]\u001b[A\n",
            " 78% 125/160 [00:07<00:02, 17.13it/s]\u001b[A\n",
            " 79% 127/160 [00:07<00:01, 17.10it/s]\u001b[A\n",
            " 81% 129/160 [00:07<00:01, 17.08it/s]\u001b[A\n",
            " 82% 131/160 [00:07<00:01, 17.34it/s]\u001b[A\n",
            " 83% 133/160 [00:07<00:01, 17.46it/s]\u001b[A\n",
            " 84% 135/160 [00:08<00:01, 17.38it/s]\u001b[A\n",
            " 86% 137/160 [00:08<00:01, 17.11it/s]\u001b[A\n",
            " 87% 139/160 [00:08<00:01, 17.28it/s]\u001b[A\n",
            " 88% 141/160 [00:08<00:01, 17.40it/s]\u001b[A\n",
            " 89% 143/160 [00:08<00:00, 17.52it/s]\u001b[A\n",
            " 91% 145/160 [00:08<00:00, 17.46it/s]\u001b[A\n",
            " 92% 147/160 [00:08<00:00, 17.27it/s]\u001b[A\n",
            " 93% 149/160 [00:08<00:00, 17.45it/s]\u001b[A\n",
            " 94% 151/160 [00:09<00:00, 17.63it/s]\u001b[A\n",
            " 96% 153/160 [00:09<00:00, 17.49it/s]\u001b[A\n",
            " 97% 155/160 [00:09<00:00, 17.06it/s]\u001b[A\n",
            " 98% 157/160 [00:09<00:00, 17.29it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.2281746119260788, 'eval_bleu': 0.0, 'eval_accuracy': 0.8885, 'eval_gen_len': 2.0, 'eval_runtime': 9.7136, 'eval_samples_per_second': 131.156, 'eval_steps_per_second': 16.472, 'epoch': 1.0}\n",
            "100% 2000/2000 [03:53<00:00,  8.24it/s]\n",
            "100% 160/160 [00:09<00:00, 16.93it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2939] 2024-01-14 20:15:55,428 >> Saving model checkpoint to out/emotion/t5_v1_1/checkpoint-2000\n",
            "[INFO|configuration_utils.py:460] 2024-01-14 20:15:55,429 >> Configuration saved in out/emotion/t5_v1_1/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:544] 2024-01-14 20:15:55,429 >> Configuration saved in out/emotion/t5_v1_1/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:2118] 2024-01-14 20:15:56,195 >> Model weights saved in out/emotion/t5_v1_1/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2024-01-14 20:15:56,197 >> tokenizer config file saved in out/emotion/t5_v1_1/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2024-01-14 20:15:56,198 >> Special tokens file saved in out/emotion/t5_v1_1/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:189] 2024-01-14 20:15:56,199 >> Copy vocab file to out/emotion/t5_v1_1/checkpoint-2000/spiece.model\n",
            "[INFO|trainer.py:2017] 2024-01-14 20:15:58,608 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2196] 2024-01-14 20:15:58,608 >> Loading best model from out/emotion/t5_v1_1/checkpoint-2000 (score: 0.8885).\n",
            "{'train_runtime': 236.643, 'train_samples_per_second': 67.612, 'train_steps_per_second': 8.452, 'train_loss': 3.418555595397949, 'epoch': 1.0}\n",
            "100% 2000/2000 [03:56<00:00,  8.45it/s]\n",
            "[INFO|trainer.py:2939] 2024-01-14 20:15:59,054 >> Saving model checkpoint to out/emotion/t5_v1_1\n",
            "[INFO|configuration_utils.py:460] 2024-01-14 20:15:59,056 >> Configuration saved in out/emotion/t5_v1_1/config.json\n",
            "[INFO|configuration_utils.py:544] 2024-01-14 20:15:59,057 >> Configuration saved in out/emotion/t5_v1_1/generation_config.json\n",
            "[INFO|modeling_utils.py:2118] 2024-01-14 20:16:00,833 >> Model weights saved in out/emotion/t5_v1_1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2420] 2024-01-14 20:16:00,835 >> tokenizer config file saved in out/emotion/t5_v1_1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2024-01-14 20:16:00,836 >> Special tokens file saved in out/emotion/t5_v1_1/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:189] 2024-01-14 20:16:00,837 >> Copy vocab file to out/emotion/t5_v1_1/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     3.4186\n",
            "  train_runtime            = 0:03:56.64\n",
            "  train_samples            =      16000\n",
            "  train_samples_per_second =     67.612\n",
            "  train_steps_per_second   =      8.452\n",
            "01/14/2024 20:16:00 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3213] 2024-01-14 20:16:00,855 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3215] 2024-01-14 20:16:00,855 >>   Num examples = 1274\n",
            "[INFO|trainer.py:3218] 2024-01-14 20:16:00,855 >>   Batch size = 8\n",
            "100% 160/160 [00:09<00:00, 16.35it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.8885\n",
            "  eval_bleu               =        0.0\n",
            "  eval_gen_len            =        2.0\n",
            "  eval_loss               =     0.2282\n",
            "  eval_runtime            = 0:00:09.85\n",
            "  eval_samples            =       1274\n",
            "  eval_samples_per_second =    129.248\n",
            "  eval_steps_per_second   =     16.232\n",
            "[INFO|modelcard.py:452] 2024-01-14 20:16:10,970 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.0}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8885}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]==4.34.1 tokenizers==0.14.1 sentencepiece==0.1.99 datasets==2.14.7 evaluate==0.4.1 sacrebleu==2.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wElD9S1qHyeG",
        "outputId": "1414224d-7ba5-4216-98bc-662863fc0339"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch]==4.34.1 in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: tokenizers==0.14.1 in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: datasets==2.14.7 in /usr/local/lib/python3.10/dist-packages (2.14.7)\n",
            "Requirement already satisfied: evaluate==0.4.1 in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: sacrebleu==2.3.2 in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]==4.34.1) (0.26.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.7) (3.9.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.1) (0.18.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.2) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.2) (4.9.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]==4.34.1) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.7) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]==4.34.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]==4.34.1) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.7) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.7) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]==4.34.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python count.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsFDepa0Inb-",
        "outputId": "ca3882c9-47b4-4a86-c289-c2fa3390429f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-14 20:31:02.813216: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-14 20:31:02.813271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-14 20:31:02.814684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-14 20:31:03.897782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Ilość poprawnych odpowiedzi: 1220\n",
            "Accuracy: 0.6084788029925187\n",
            "Categorize this sentence with this options : joy, sadness, love, anger, fear, surprise:i woke up feeling fine\n",
            "joy\n",
            "Categorize this sentence with this options : joy, sadness, love, anger, fear, surprise:i now feel compromised and skeptical of the value of every unit of work i put in\n",
            "sadness\n",
            "Categorize this sentence with this options : joy, sadness, love, anger, fear, surprise:i have seen heard and read over the past couple of days i am left feeling impressed by more than a few companies\n",
            "surprise\n"
          ]
        }
      ]
    }
  ]
}